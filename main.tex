\documentclass[t, 10pt, handout, aspectratio=169]{beamer}
\usepackage{lipsum}
\usepackage{tikz}
\usepackage{pgfplots, pgfplotstable}
\usepackage{booktabs}
\usepackage{amsmath,bm,amstext}
\usepackage{mdframed}
\usepackage{mleftright}



\pgfplotsset{compat=1.14}

%\usetheme[framenumber,totalframenumber]{QU}
\usetheme[color=blue,framenumber,totalframenumber, footline, footertext]{KU}


\title[Introduction to Tensor]{Introduction to \texttt{Tensor}}
\subtitle{Intelligent Computing for Computational Intelligence \\
in post Moore’s Law era!}

\author[yanglet]{Xiao-Yang Liu\\
\url{www.tensorlet.com}}
\institute[CU]{Columbia University}

\footertext{\textcolor{red}{\url{www.tensorlet.com}}}


\date[\number\month/\number\day/\number\year]{\today}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Agenda}
\begin{itemize}
    \large \item \textcolor{red}{Background}
    \large \item {Tensor Decompositions (CP, Tucker, and Tensor-Train/Tensor-Ring)}
    \large \item{Transform-based Tensor Model and Applications}
    \large \item{Tensor Computations (cuTensor, TensorDeC$++$)}
\end{itemize}
\end{frame}

\begin{frame}{Background}
\large Multidimensional data of exceedingly huge volume, variety and structural richness become ubiquitous across disciplines in engineering and data science:
\begin{itemize}
    \item multimedia data like speech and video
    \item remote sensing data
    \item medical and biological data
    \item seismic data
\end{itemize}
\vskip 0.2\margin
\large Some data can have more meaningful representation using multi-way arrays -- \textbf{tensor}, rather than matrices (two-way arrays).
\end{frame}

\begin{frame}{What is tensor?}
\vskip -1ex
\begin{figure}
	\centering  
	\includegraphics[height=0.7\paperheight]{figs/tensor_shape}
	\label{fig:tensor_shape}
\end{figure}
\end{frame}

\begin{frame}{What is tensor?}
\vskip -1ex
\begin{figure}
	\centering  
	\includegraphics[height=0.7\paperheight]{figs/tensor_sample}
	\label{fig:tensor_sample}
\end{figure}
\end{frame}

\begin{frame}{Tensor fibers}
\vskip -3ex
\begin{figure}
	\centering  
	\includegraphics[width=\linewidth]{figs/tensor_fibers}
	\label{fig:tensor_fibers}
\end{figure}
\end{frame}

\begin{frame}{Tensor slices}
\begin{figure}
	\centering  
	\includegraphics[width=\linewidth]{figs/tensor_slices}
	\label{fig:tensor_slices}
\end{figure}
\end{frame}

\begin{frame}{Tensor unfolding}
\vskip -3ex
\begin{columns}[c]  %开始进入分栏环境，居中设置
\column{0.4\linewidth}  
\begin{figure}
	\centering  
	\includegraphics[height=0.75\paperheight]{figs/tensor_unfolding}
	\label{fig:tensor_unfolding}
\end{figure}

\column{0.6\linewidth}  
\begin{figure}
	\centering  
	\includegraphics[width=\linewidth]{figs/tensor_unfolding2}
	\label{fig:tensor_unfolding2}
\end{figure}
\centering
$\mathbf{A}_{(i)}$ means mode-$i$ unfolding.
\end{columns}  %分栏环境结束

\end{frame}



\begin{frame}{Agenda}
\begin{itemize}
    \large \item {Background}
    \large \item \textcolor{red}{Tensor Decompositions (CP, Tucker, and Tensor-Train/Tensor-Ring)}
    \large \item{Transform-based Tensor Model and Applications}
    \large \item{Tensor Computations (cuTensor, TensorDeC$++$)}
\end{itemize}
\end{frame}

\begin{frame}{CP Decomposition}
\vskip -1ex
\begin{figure}[t]
	\centering  
	\includegraphics[width=\linewidth]{figs/cp_arch}
	\label{fig:cp_arch}
\end{figure}
\begin{columns}
\column{0.5\linewidth}
\vskip -10ex
\begin{align*}
\underline{\mathbf{X}}&\approx\sum_{r=1}^{R}\lambda_{r}\mathbf{b}_{r}^{(1)}\circ\mathbf{b}_{r}^{(2)}\circ\cdots\circ\mathbf{b}_{r}^{(N)} \\
&=\underline{\mathbf{\Lambda}}\times_{1}\mathbf{B}^{(1)}\times_{2}\mathbf{B}^{(2)}\cdots\times_{N}\mathbf{B}^{(N)} \\
&=[\![\mathbf{\Lambda};\mathbf{B}^{(1)},\mathbf{B}^{(1)},\cdots,\mathbf{B}^{(N)}]\!]
\end{align*}

\column{0.5\linewidth}
\vskip -8ex
\begin{align*}
\mathbf{X}_{(1)}&=\mathbf{A}\mathbf{\Lambda}(\mathbf{C}\odot\mathbf{B})^{T}+\mathbf{E}_{(1)} \\
\mathbf{X}_{(2)}&=\mathbf{B}\mathbf{\Lambda}(\mathbf{C}\odot\mathbf{A})^{T}+\mathbf{E}_{(2)} \\
\mathbf{X}_{(3)}&=\mathbf{C}\mathbf{\Lambda}(\mathbf{B}\odot\mathbf{A})^{T}+\mathbf{E}_{(3)}
\end{align*}

\end{columns}
\end{frame}

\begin{frame}{CP Decomposition}
\large
\begin{table}
\begin{tabular}{l | l}
\textbf{Name} & \textbf{Proposed by} \\
\hline \hline
Polyadic form of a tensor & Hitchcock, 1927 \\ 
PARAFAC (parallel factors) & Harshman, 1970\\
CANDECOMP or CAND (canonical decomposition) & Carroll and Chang, 1970\\
Topographic components model & Mocks, 1988 \\
CP (CANDECOMP/PARAFAC) & Kiers, 2000
\end{tabular}
\caption{Some of the many names for the CP decomposition.}
\end{table}
\end{frame}

\begin{frame}{Tucker Decomposition}
\vskip -2ex
\begin{figure}[t]
	\centering  
	\includegraphics[width=0.95\linewidth]{figs/tucker_arch}
	\label{fig:tucker_arch}
\end{figure}
\vskip -2.5ex
\begin{mdframed}[backgroundcolor=brown!20,roundcorner=8pt,leftmargin=60pt,rightmargin=60pt]
\centering
$
\underline{\mathbf{Y}}=\underline{\mathbf{G}}\times_{1}\mathbf{A}\times_{2}\mathbf{B}\times_{3}\mathbf{C}+\mathbf{E}=[\![\mathbf{G};\mathbf{A},\mathbf{B},\mathbf{C}]\!]+\underline{\mathbf{E}}
$
\end{mdframed}
\vskip -0.5ex
\begin{align*}
\mathbf{X}_{(1)}&\approx\mathbf{A}\mathbf{G}_{(1)}(\mathbf{C}\otimes\mathbf{B})^{T}\\
\mathbf{X}_{(2)}&\approx\mathbf{B}\mathbf{G}_{(2)}(\mathbf{C}\otimes\mathbf{A})^{T}\\
\mathbf{X}_{(3)}&\approx\mathbf{C}\mathbf{G}_{(3)}(\mathbf{B}\otimes\mathbf{A})^{T}
\end{align*}
\end{frame}

\begin{frame}{Tucker Decomposition}
\large
\begin{table}
\begin{tabular}{l | l}
Name & Proposed by \\
\hline \hline
Three-mode factor analysis (3MFA/Tucker3) & Tucker, 1966 \\ 
Three-mode PCA (3MPCA) &  Kroonenberg and De Leeuw, 1980\\
N-mode PCA & Kapteyn et al., 1986 \\
Higher-order SVD (HOSVD)  & De Lathauwer et al., 2000 \\
N-mode SVD & Vasilescu and Terzopoulos, 2002
\end{tabular}
\caption{Names for the Tucker decomposition (some specific to three-way and some for N-way).}
\end{table}
\end{frame}

\begin{frame}{Tensor Train Decomposition}
\large
\begin{block}{Low-rank Decomposition}
$$
\begin{aligned}
&\mathbf{A}=\mathbf{G}_{1}\mathbf{G}_{2}  \\
&\mathbf{G}_{1}\text{: collection of rows, }\mathbf{G}_{2}\text{: collection of columbs}\\
&\mathbf{A}_{i_1i_2}=\underbrace{\mathbf{G}_1(i_1)}_{1\times R}\underbrace{\mathbf{G}_2(i_2)}_{R\times 1}
\end{aligned}
$$
\end{block}
\begin{figure}
	\centering  
	\includegraphics[width=0.45\linewidth]{figs/tt_lowrank.png}
	\label{fig:tt_lowrank}
\end{figure}
\end{frame}

\begin{frame}{Tensor Train Decomposition}
\large
\begin{block}{TT Form}
$$
\underline{\mathbf{A}}_{i_1i_2\cdots i_N}=\underbrace{\mathbf{G}_{1}(i_1)}_{1\times R}\underbrace{\mathbf{G}_{2}(i_2)}_{R\times R}\cdots\underbrace{\mathbf{G}_{N}(i_N)}_{R\times 1}
$$
\end{block}
\begin{figure}
	\centering  
	\includegraphics[width=0.9\linewidth]{figs/tt_form_example.png}
	\label{fig:tt_form_example}
\end{figure}
\end{frame}

\begin{frame}{Tensor Ring Decomposition}
\large

\begin{block}{TR Form}
$$
\begin{aligned}
\underline{\mathbf{A}}_{i_1i_2\cdots i_N}=\text{Tr}\{\mathbf{Z}_1(i_1)\mathbf{Z}_2(i_2)\cdots\mathbf{Z}_N(i_N)\}
=\text{Tr}\left\{\prod_{k=1}^{N}\mathbf{Z}_k(i_k)\right\}
\end{aligned}
$$
\end{block}
\begin{figure}
	\centering  
	\includegraphics[width=0.55\linewidth]{figs/tensor_ring_arch.png}
	\label{fig:tensor_ring_arch}
\end{figure}
\end{frame}

\begin{frame}{Agenda}
\begin{itemize}
    \large \item {Background}
    \large \item {Tensor Decompositions (CP, Tucker, and Tensor-Train/Tensor-Ring)}
    \large \item \textcolor{red}{Transform-based Tensor Model and Applications}
    \large \item{Tensor Computations (cuTensor, TensorDeC$++$)}
\end{itemize}
\end{frame}

\begin{frame}{Low-tubal-rank Model}
\vskip -1ex
\begin{block}{Notation}
$$\vec{\mathcal{A}}_i\equiv\mathcal{A}(:,i,:),~~~~\mathcal{A}^{(j)}\equiv\mathcal{A}(:,:,j),~~~~\hat{\mathcal{A}} := \texttt{fft}(\mathcal{A}, [], 3)$$
\vskip -1ex
$$
\texttt{bcirc}(\mathcal{A})=\left[
  \begin{matrix}
   A^{(1)} & A^{(n)} & A^{(n-1)} & \cdots &A^{(2)} \\
   A^{(2)} & A^{(1)} & A^{(n)} & \cdots &A^{(3)} \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
    A^{(n)} & A^{(n-1)} & \cdots &A^{(2)} & A^{(1)}
  \end{matrix}
  \right]
$$
$$
\texttt{unfold}(\mathcal{A})=\left[
  \begin{matrix}
    A^{(1)} \\
    A^{(2)} \\
    \vdots \\
    A^{(n)}
  \end{matrix}
\right], ~~~~~~~~\texttt{fold}(\texttt{unfold}(\mathcal{A}))=\mathcal{A}
$$
\end{block}
\end{frame}

\begin{frame}{Low-tubal-rank Model}
\begin{block}{t-Product}
$$\mathcal{A}*\mathcal{B}=\texttt{fold}(\texttt{bcirc}(\mathcal{A})\cdot\texttt{unfold}(\mathcal{B}))$$
\end{block}
\textbf{Example}. Let $\mathcal{A}\in\mathbb{R}^{3\times 2 \times 2}$ with frontal faces
$$
%\renewcommand\arraystretch{1.3}
A^{(1)}=\left[
\begin{array}{c c}
  1 & 0 \\
  0 & 2 \\
  -1 & 3
\end{array}
\right]~~~~
\text{and}~~~~A^{(2)}=\mleft[
\begin{array}{c c}
  -2 & 1 \\
  -2 & 7 \\
  0 & -1
\end{array}
\mright],
$$
and let $\vec{\mathcal{B}}\in\mathbb{R}^{2\times 1 \times 2}$ with frontal faces
$$
B^{(1)}=
\left[\begin{matrix}
3\\
-1
\end{matrix}\right]
~~~~\text{and}~~~~
B^{(2)}=
\left[\begin{matrix}
-2\\
-3
\end{matrix}\right].
$$
\end{frame}

\begin{frame}{Low-tubal-rank Model}
$$
\begin{aligned}
\mathcal{A}*\vec{\mathcal{B}} & =\texttt{fold}\left(
\left[\begin{array}{cc|cc}
1 & 0 & -2 & 1\\
0 & 2 & -2 & 7\\
-1 & 3 & 0 & -1\\
\hline
-2 & 1 & 1 & 0\\
-2 & 7 & 0 & 2\\
0 & -1 & -1 & 3
\end{array}\right]
\left[\begin{array}{c}
3\\
-1\\
\hline
-2\\
-3
\end{array}\right]
\right)\\
& = \texttt{fold}\left(
\left[\begin{array}{c}
4\\
-19\\
-3\\
\hline
-9\\
-19\\
-6
\end{array}\right]
\right)\in\mathbb{R}^{3\times 1 \times 2}
\end{aligned}
$$
is a $3 \times 1 \times 2$ tensor. In other words, in this example, $\vec{\mathcal{C}} := \mathcal{A} * \vec{\mathcal{B}}$ is a $3\times 2$ matrix, oriented as a lateral slice of a third-order tensor.
\end{frame}

\begin{frame}{Low-tubal-rank Model}
\begin{block}{t-Linear combinations}
~~~~Given $k$ tubal scalars $\mathbf{c}_j\in\mathbb{R}^{1\times 1\times n},~j=1,2,\cdots,k$, a t-linear combination of $\vec{\mathcal{X}}_j\in \mathbb{R}^{m\times 1\times n},~j=1,2,\cdots,k$ is defined as
$$
\vec{\mathcal{X}}_1*\mathbf{c}_1 + \vec{\mathcal{X}}_2 * \mathbf{c}_2 + \cdots + \vec{\mathcal{X}}_k * \mathbf{c}_k \equiv \mathcal{X} * \vec{\mathcal{C}}
$$
where
$$
\mathcal{X} := \left[\vec{\mathcal{X}}_1, \vec{\mathcal{X}}_2, \cdots, \vec{\mathcal{X}}_k\right],~~~~\vec{\mathcal{C}}:=\left[\begin{matrix}
\mathbf{c}_1\\
\mathbf{c}_2\\
\vdots\\
\mathbf{c}_k
\end{matrix}\right].
$$
\end{block}
\textbf{Example}. Using $\mathcal{A}\in\mathbb{R}^{3\times 2 \times 2}$ and $\vec{\mathcal{B}}\in\mathbb{R}^{2\times 1\times 2}$ from the previous example, we see that

\end{frame}

\begin{frame}{Low-tubal-rank Model}
$$
\begin{aligned}
\mathcal{A}*\vec{\mathcal{B}} & = \vec{\mathcal{A}}_1 * \mathbf{b}_{11} + \vec{\mathcal{A}}_2 * \mathbf{b}_{21} \\
& = \texttt{fold}\left(
\left[\begin{array}{c}
7\\
4\\
-3\\
\hline
-8\\
-6\\
2
\end{array}\right]
\right) + \texttt{fold}\left(
\left[\begin{array}{c}
-3\\
-23\\
0\\
\hline
-1\\
-13\\
-8
\end{array}\right]
\right)=\texttt{fold}\left(
\left[\begin{array}{c}
4\\
-19\\
-3\\
\hline
-9\\
-19\\
-6
\end{array}\right]
\right)
\end{aligned}
$$
Thus, $\vec{\mathcal{C}} := \mathcal{A} * \vec{\mathcal{B}}$ is a t-linear combination of the lateral slices of $\mathcal{A}$.
\end{frame}

\begin{frame}{Low-tubal-rank Model}

\begin{block}{Observation}
~~~~Given $\mathbf{a}, \mathbf{b} \in \mathbb{K}_n$, $\mathbf{a} * \mathbf{b}$ can be computed as
$$
\mathbf{a} * \mathbf{b} := \texttt{ifft}(\hat{\mathbf{a}} \odot \hat{\mathbf{b}}, [], 3),
$$
where $\odot$ of two tubal scalars means pointwise multiplication.\\
~~~~Factorizations of $\mathcal{A}$ are created (implicitly) by applying the appropriate matrix factorization to each of the $\hat{\mathcal{A}}^{(i)}$
$$
\mathcal{A}=\mathcal{Q} * \mathcal{R} \Longleftrightarrow \hat{\mathcal{A}}^{(i)}=\hat{\mathcal{Q}}^{(i)}\hat{\mathcal{R}}^{(i)}.
$$
\end{block}
\end{frame}

\begin{frame}{Low-tubal-rank Model}
\begin{block}{t-SVD}
$$
\mathcal{A}=\mathcal{U}*\mathcal{S}*\mathcal{V}^{T}=\sum_{i=1}^{\text{min}(l,m)}\vec{\mathcal{U}}_i*\mathbf{s}_i*\vec{\mathcal{V}}_i^T, ~~~~\mathbf{s}_i :=\mathcal{S}(i,i,:)
$$
\end{block}
\vskip -2ex
\begin{figure}
	\centering  
	\includegraphics[width=0.7\linewidth]{figs/tsvd_arch.png}\\
	The t-SVD of an $l \times m \times n$ tensor
	\label{fig:tsvd_arch}
\end{figure}
\end{frame}

\begin{frame}{Tubal-tensor Sparse Coding}
\begin{block}{Tubal-tensor Linear Combination}
A two-dimensional image of size $m \times k$ is represented by a third-order tensor $\mathcal{X}\in \mathbb{R}^{m\times 1 \times k}$, which can be approximated by the t-product between $\mathcal{D}\in\mathbb{R}^{m\times r\times k}$ and $\mathcal{C}\in\mathbb{R}^{r\times 1 \times k}$ as
$$
\begin{aligned}
\mathcal{X} & = \mathcal{D} * \mathcal{C}\\
& = \mathcal{D}(:,1,:)*\mathcal{C}(1,1,:) + \mathcal{D}(:,2,:)*\mathcal{C}(2,1,:) + \cdots + \mathcal{D}(:,r,:)*\mathcal{C}(r,1,:)
\end{aligned}
$$
\end{block}
\vskip -2ex
\begin{figure}
	\centering  
	\includegraphics[width=0.5\linewidth]{figs/tubaltensor_arch.png}\\
	Tubal-tensor sparse coding model is based on circular convolution operation
	\label{fig:tubaltensor_arch}
\end{figure}

\end{frame}

\begin{frame}{Tubal-tensor Sparse Coding}
\begin{block}{Tubal-tensor Sparse Representation}
~~~~A third-order tensor $\mathcal{X} \in \mathbb{R}^{m\times n\times k}$ is presented by $n$ images of size $m \times k$. Let $\mathcal{D} \in \mathbb{R}^{m\times r\times k}$ be the tensor dictionary, where each lateral slice $\mathcal{D}(:,j,:)$ represents a tensor basis, and $\mathcal{C}\in \mathbb{R}^{r\times n \times k}$ be the tensor corresponding representations. Each image $\mathcal{X}(:,j,:)$ is approximated by a sparse t-linear combination of those tensor bases. Tubal-tensor sparse coding (TubSC) model can be formulated as
$$
\begin{aligned}
\min_{\mathcal{D},\mathcal{C}}&~~~\frac{1}{2}\|\mathcal{X}-\mathcal{D}*\mathcal{C}\|_{F}^{2}+\beta\|\mathcal{C}\|_1 \\
\text{s.t.} &~~~ \|\mathcal{D}(:,j,:)\|_F^2\le 1, j=1,2,\cdots,r.
\end{aligned}
$$
\end{block}
~~~~~TubSC model can be solved alternately by \textbf{tensor coefficients learning} and \textbf{tensor dictionary learning}.
\end{frame}

\begin{frame}{Tubal-tensor Sparse Coding}
\begin{block}{Tensor Coefficients Learning}
$$
\min_{\mathcal{C}}~~\frac{1}{2}\|\mathcal{X}-\mathcal{D}*\mathcal{C}\|_F^2+\beta\|\mathcal{C}\|_1
$$
~~~~According to low-tubal-tensor model, the problem can be transformed to
$$
\min_{\texttt{unfold}(\mathcal{C})}~~\frac{1}{2}\|\texttt{unfold}(\mathcal{X})-\texttt{bcirc}(\mathcal{D})\cdot\texttt{unfold}(\mathcal{C})\|_F^2+\beta\|\texttt{unfold}(\mathcal{C})\|_1.
$$
It can be solved by Iterative Shrinkage Thresholding algorithm based on Tensor (\textbf{ISTT}), which is rewritten as
$$
\min_{\mathcal{C}}~~f(\mathcal{C})+\beta g(\mathcal{C}),
$$
where $f(\mathcal{C}) = \frac{1}{2}\|\mathcal{X}-\mathcal{D}*\mathcal{C}\|_F^2,~\text{and}~g(\mathcal{C}) = \|\mathcal{C}\|_1$.
\end{block}
\end{frame}

\begin{frame}{Tubal-tensor Sparse Coding}
\begin{block}{Tensor Dictionary Learning}
\vskip -1ex
$$
\begin{aligned}
\min_{\mathcal{D}}&~~~\frac{1}{2}\|\mathcal{X}-\mathcal{D}*\mathcal{C}\|_{F}^{2}\\
\text{s.t.} &~~~ \|\mathcal{D}(:,j,:)\|_F^2\le 1, j=1,2,\cdots,r.
\end{aligned}
$$
~~~~We transform this problem into the frequecy domain:
\vskip -1ex
$$
\begin{aligned}
\min_{\hat{\mathcal{D}}^{(l)}}&~~~\sum_{l=1}^{k}\|\hat{\mathcal{X}}^{(l)}-\hat{\mathcal{D}}^{(l)}\hat{\mathcal{C}}^{(l)}\|_F^2,l=1,2,\cdots,k\\
\text{s.t.}&~~~\sum_{l=1}^{k}\|\hat{\mathcal{D}}^{(l)}(:,j)\|_F^2\le k, j=1,2,\cdots,r.
\end{aligned}
$$
~~~~Then adopt the Lagrange dual (Lee et al. 2007) to solve the dual variables by Newton’s algorithm.
\end{block}
\end{frame}

\begin{frame}{Agenda}
\begin{itemize}
    \large \item {Background}
    \large \item {Tensor Decompositions (CP, Tucker, and Tensor-Train/Tensor-Ring)}
    \large \item {Transform-based Tensor Model and Applications}
    \large \item \textcolor{red}{Tensor Computations (cuTensor, TensorDeC$++$)}
\end{itemize}
\end{frame}

\begin{frame}{cuTensor-tubal (GPU)}
\large
This library is a general approach to compute low-tubal-rank tensor operations in the frequency domain on GPUs.
\begin{enumerate}
  \item Obtain the frequency domain representation of the input tensor by performing Fourier transform along the third dimension (called tube-wise DFT) on the GPU;
  \item In the frequency domain, the tensor operations are separated into multiple independent complex matrix computations that possess strong parallelism;
  \item Converting the frequency domain results back to the time domain through inverse Fourier transform along the third dimension on the GPU (called tube-wise inverse DFT).
\end{enumerate}
\end{frame}

\begin{frame}{cuTensor-tubal (GPU)}

\begin{table}
$\begin{array}{l | l | l}
\textbf{Operation} & \textbf{Input} & \textbf{Output} \\
\hline \hline
\text{t-FFT} & \mathcal{A}\in\mathbb{R}^{m\times n\times k} & \hat{\mathcal{A}}\in\mathbb{C}^{m\times n\times k} \\
\text{inverse t-FFT} & \hat{\mathcal{A}}\in\mathbb{C}^{m\times n\times k} & \mathcal{A}\in\mathbb{R}^{m\times n\times k} \\
\text{t-product} & \mathcal{A}\in\mathbb{R}^{m\times l\times k},\mathcal{B}\in\mathbb{R}^{l\times n\times k} & \mathcal{C}\in\mathbb{R}^{m\times n\times k} \\
\text{t-SVD} & \mathcal{T}\in\mathbb{R}^{m\times n\times k} & \mathcal{U}\in\mathbb{R}^{m\times m\times k},\mathcal{V}\in\mathbb{R}^{n\times n\times k}, \mathcal{\Theta}\in\mathbb{R}^{m\times n\times k} \\
\text{t-QR} & \mathcal{T}\in\mathbb{R}^{m\times n\times k} & \mathcal{Q}\in\mathbb{R}^{m\times m\times k}, \mathcal{R}\in\mathbb{R}^{m\times n\times k} \\
\text{t-inverse} &\mathcal{T}\in\mathbb{R}^{n\times n\times k} &\mathcal{T}^{-1}\in\mathbb{R}^{n\times n\times k} \\
\text{t-normalization} & \mathcal{T}\in\mathbb{R}^{m\times 1\times k} & \mathcal{T}\in\mathbb{R}^{m\times 1\times k}
\end{array}$
\caption{Seven tensor operations in the cuTensor-tubal library}
\end{table}
\end{frame}

\begin{frame}{cuTensor-tubal (GPU)}
\large
\begin{block}{Key Challenges}

\begin{itemize}
  \item Data Transfer Between the CPU and GPU 
  \item Alternative Access to Tube and Slice Data Structures
  \item Parallelizing the Fourier Transforms and Matrix Computations
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{cuTensor-tubal (GPU)}
\framesubtitle{Efficient Data Transfer}
\vskip -2ex
\begin{figure}
	\centering  
	\includegraphics[width=\linewidth]{figs/cutensor_datatrans.png}\\
	Overlapping data transfer with computations
	\label{fig:cutensor_datatrans}
\end{figure}
\end{frame}

\begin{frame}{cuTensor-tubal (GPU)}
\framesubtitle{ Uniform Memory Access to Tube and Slice Data Structures}
\vskip -2ex
\vfill
\begin{figure}
	\centering  
	\includegraphics[width=0.5\linewidth]{figs/cutensor_datastorage.png}\hfill  \includegraphics[width=0.5\linewidth]{figs/cutensor_datastructure.png}\\
	Tensors are stored as a 1D array in memory\hfill Data structures in tensor computations~~~~~
	\label{fig:cutensor_datastructure}
\end{figure}
\vfill
\end{frame}

\begin{frame}{cuTensor-tubal (GPU)}
\framesubtitle{Parallelizing the Fourier Transforms and Matrix Computations}

\begin{table}
$\begin{array}{l | l | l |l}
\textbf{Operation} & \textbf{\#(FFT operations)} & \textbf{\#(Matrix Operation)} & \textbf{\#(inverse FFT operation)}\\
\hline \hline
\text{t-FFT} & m \times n & \text{None} & m \times n \\
\text{inverse t-FFT} & m \times n & \text{None} & m \times n \\
\text{t-product} & m \times l + l\times n & k & m \times n \\
\text{t-SVD} & m \times n & k & m \times n + n\times n + m\times n \\
\text{t-QR} & m \times n & k & m \times m + m \times n \\
\text{t-inverse} & n \times n & k & n \times n \\
\text{t-normalization} & m & k & m 
\end{array}$
\caption{Seven tensor operations in the cuTensor-tubal library}
\end{table}
\end{frame}

\begin{frame}{cuTensor-tubal (GPU)}
\begin{figure}
	\centering  
	\includegraphics[width=0.9\linewidth]{figs/cutensor_workflow.png} \\
	System workflow of the cuTensor-tubal library
	\label{fig:cutensor_workflow}
\end{figure}
\begin{figure}
	\centering  
	\includegraphics[width=0.6\linewidth]{figs/cutensor_memoryaccess.png} \\
	Memory access operators
	\label{fig:cutensor_memoryaccess}
\end{figure}
\end{frame}

\begin{frame}{TenDeC++ (CPU)}
\begin{columns}
\column{0.55\linewidth}
TenDeC++ is a new library for tensor decompositions in C++, in which a novel underlying technology \textbf{PointerDefomer} leveraging he unique pointer is proposed to further explore potentials of C++. TenDeC++ supports 
\begin{itemize}
\item Canonical Polyadic
\item Tucker Decomposition
\item Tensor-train Decomposition
\item t-SVD
\end{itemize}
Compared with Tensorly in Python and TensorLab in MATLAB, TenDeC++ reduces more than \textbf{83.7\%}, \textbf{53.3\%} decomposition time, and supports \textbf{2.5$\times$}, \textbf{2$\times$} of tensor.
\column{0.45\linewidth}
\vskip -2ex
\begin{figure}
	\centering  
	\includegraphics[width=\linewidth]{figs/tendecpp_arch.png} \\
	System architecture of the TenDeC++ library
	\label{fig:tendecpp_arch}
\end{figure}
\end{columns}
\vfill
\end{frame}

\begin{frame}{TenDeC++ (CPU)}
\framesubtitle{PointerDeformer}
A 3D tensor is stored as a 1D array in memory. Accessing these data with different sequences can form size-specific matrices including three mode-$n$views: column-major, row-major, and concatenation views. These virtual views motivate us to design PointerDeformer to skip the time-consuming unfolding operation in C++.
\begin{figure}
	\centering  
	\includegraphics[width=\linewidth]{figs/pointerdeformer_example.png}
	\label{fig:pointerdeformer_example}
\end{figure}
\end{frame}

\begin{frame}{TenDeC++ (CPU)}
\framesubtitle{Optimized Basic Tensor Operation: $n$-mode Product}
Compare with traditional process, the optimized process does not need the time-consuming unfold/fold operations. Instead, PointerDeformer achieves the virtual transformation by accessing the data with specific sequence in memory.
\begin{figure}
	\centering  
	\includegraphics[width=0.6\linewidth]{figs/tendecpp_optimized.png}
	\label{fig:tendecpp_optimized}
\end{figure}
\end{frame}

\begin{frame}{TenDeC++ (CPU)}
\framesubtitle{Other Acceleration Techniques}
\large
\begin{itemize}
\item Exploit Symmetry with PointerDefomer
\item Exploit Conjugate Symmetry for t-SVD Decomposition
\end{itemize}
\end{frame}

\begin{frame}{TenDeC++ (CPU)}
\framesubtitle{Performance}
\vskip -4.6ex
\begin{columns}
\column{0.5\linewidth}
\begin{figure}
	\centering  
	\includegraphics[width=\linewidth]{figs/tendecpp_eval1.png}\\
	Running time of Tucker (TT) decomposition
	\label{fig:tendecpp_eval1}
\end{figure}
\column{0.5\linewidth}
\vskip -0.6ex
\begin{figure}
	\centering  
	\includegraphics[width=\linewidth]{figs/tendecpp_eval2.png}\\
	Running time of t-SVD
	\label{fig:tendecpp_eval2}
\end{figure}
\end{columns}
\end{frame}
\end{document}
